{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Crane-Droesch's algorithm in pytorch, for better reproducibility.\n",
    "See the original paper [here](https://iopscience.iop.org/article/10.1088/1748-9326/aae159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "from patsy import dmatrices\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>year</th>\n",
       "      <th>cornyield</th>\n",
       "      <th>cottonyield</th>\n",
       "      <th>soyyield</th>\n",
       "      <th>prec_101</th>\n",
       "      <th>prec_102</th>\n",
       "      <th>prec_103</th>\n",
       "      <th>prec_104</th>\n",
       "      <th>prec_105</th>\n",
       "      <th>...</th>\n",
       "      <th>tMax_1223</th>\n",
       "      <th>tMax_1224</th>\n",
       "      <th>tMax_1225</th>\n",
       "      <th>tMax_1226</th>\n",
       "      <th>tMax_1227</th>\n",
       "      <th>tMax_1228</th>\n",
       "      <th>tMax_1229</th>\n",
       "      <th>tMax_1230</th>\n",
       "      <th>tMax_1231</th>\n",
       "      <th>logcornyield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19001</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.6</td>\n",
       "      <td>0.012033</td>\n",
       "      <td>0.589944</td>\n",
       "      <td>0.016872</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.547232</td>\n",
       "      <td>-10.325850</td>\n",
       "      <td>-7.136567</td>\n",
       "      <td>-7.685323</td>\n",
       "      <td>-6.439754</td>\n",
       "      <td>1.537972</td>\n",
       "      <td>6.898899</td>\n",
       "      <td>4.773990</td>\n",
       "      <td>2.318576</td>\n",
       "      <td>3.602777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19003</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>29.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.012352</td>\n",
       "      <td>0.466833</td>\n",
       "      <td>0.017530</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.751188</td>\n",
       "      <td>-8.839404</td>\n",
       "      <td>-7.620637</td>\n",
       "      <td>-7.860257</td>\n",
       "      <td>-6.613001</td>\n",
       "      <td>1.977630</td>\n",
       "      <td>5.981259</td>\n",
       "      <td>4.881736</td>\n",
       "      <td>2.989506</td>\n",
       "      <td>3.377588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19005</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.010945</td>\n",
       "      <td>0.290628</td>\n",
       "      <td>0.375549</td>\n",
       "      <td>0.081220</td>\n",
       "      <td>0.431489</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.687468</td>\n",
       "      <td>-13.181018</td>\n",
       "      <td>-5.470778</td>\n",
       "      <td>-8.814853</td>\n",
       "      <td>-11.140276</td>\n",
       "      <td>-1.914674</td>\n",
       "      <td>3.245067</td>\n",
       "      <td>2.077545</td>\n",
       "      <td>2.868546</td>\n",
       "      <td>3.899950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19007</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.011618</td>\n",
       "      <td>0.578579</td>\n",
       "      <td>0.711192</td>\n",
       "      <td>0.190579</td>\n",
       "      <td>0.011618</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.857291</td>\n",
       "      <td>-10.774097</td>\n",
       "      <td>-5.085959</td>\n",
       "      <td>-7.094800</td>\n",
       "      <td>-5.920215</td>\n",
       "      <td>2.955707</td>\n",
       "      <td>5.388830</td>\n",
       "      <td>4.523667</td>\n",
       "      <td>10.219614</td>\n",
       "      <td>3.367296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19009</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>50.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.009158</td>\n",
       "      <td>0.599447</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.321472</td>\n",
       "      <td>-10.493649</td>\n",
       "      <td>-8.044870</td>\n",
       "      <td>-8.837232</td>\n",
       "      <td>-7.460020</td>\n",
       "      <td>-0.166930</td>\n",
       "      <td>5.955386</td>\n",
       "      <td>4.289409</td>\n",
       "      <td>1.768050</td>\n",
       "      <td>3.916015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1466 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips    year  cornyield  cottonyield  soyyield  prec_101  prec_102  \\\n",
       "0  19001  1951.0       36.7          NaN      21.6  0.012033  0.589944   \n",
       "1  19003  1951.0       29.3          NaN      16.4  0.012352  0.466833   \n",
       "2  19005  1951.0       49.4          NaN      12.5  0.010945  0.290628   \n",
       "3  19007  1951.0       29.0          NaN      18.9  0.011618  0.578579   \n",
       "4  19009  1951.0       50.2          NaN      24.5  0.009158  0.599447   \n",
       "\n",
       "   prec_103  prec_104  prec_105      ...       tMax_1223  tMax_1224  \\\n",
       "0  0.016872  0.013338  0.012083      ...       -7.547232 -10.325850   \n",
       "1  0.017530  0.014132  0.012457      ...       -6.751188  -8.839404   \n",
       "2  0.375549  0.081220  0.431489      ...      -12.687468 -13.181018   \n",
       "3  0.711192  0.190579  0.011618      ...       -6.857291 -10.774097   \n",
       "4  0.009177  0.009167  0.009159      ...       -9.321472 -10.493649   \n",
       "\n",
       "   tMax_1225  tMax_1226  tMax_1227  tMax_1228  tMax_1229  tMax_1230  \\\n",
       "0  -7.136567  -7.685323  -6.439754   1.537972   6.898899   4.773990   \n",
       "1  -7.620637  -7.860257  -6.613001   1.977630   5.981259   4.881736   \n",
       "2  -5.470778  -8.814853 -11.140276  -1.914674   3.245067   2.077545   \n",
       "3  -5.085959  -7.094800  -5.920215   2.955707   5.388830   4.523667   \n",
       "4  -8.044870  -8.837232  -7.460020  -0.166930   5.955386   4.289409   \n",
       "\n",
       "   tMax_1231  logcornyield  \n",
       "0   2.318576      3.602777  \n",
       "1   2.989506      3.377588  \n",
       "2   2.868546      3.899950  \n",
       "3  10.219614      3.367296  \n",
       "4   1.768050      3.916015  \n",
       "\n",
       "[5 rows x 1466 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = \"/Users/max/Documents/climate_and_agriculture/code/\" # set your own directory here\n",
    "df = pd.read_csv(base + \"data/iowa_weather.csv\")\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "field = 'cornyield'\n",
    "df['log' + field] = np.log(df[field])\n",
    "df = df.dropna(axis=0, subset=['log'+ field])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the covariates\n",
    "y = df[[\"log\" + field]]\n",
    "fe = df[[\"fips\"]]\n",
    "parametric = None\n",
    "nonparametric = df[df.columns[pd.Series(df.columns).str.contains(\"_\")]]\n",
    "NP = torch.tensor(nonparametric.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupmeans(G, X):\n",
    "    gm = pd.concat([G, X], axis = 1).groupby(G.columns[0], axis = 0).mean()\n",
    "    GM = np.empty(shape = X.shape)\n",
    "    for j in np.unique(G.values):\n",
    "        idx = np.where(j == G.values)[0]\n",
    "        toput = np.repeat(gm.loc[j,].values.reshape(1,GM.shape[1]), repeats = len(idx), axis = 0)\n",
    "        GM[idx,:] = toput\n",
    "    return GM\n",
    "\n",
    "# ols function\n",
    "def ols(X, y, lam = 0, parapen = None):\n",
    "    # default value is unpenalized\n",
    "    if parapen is None:\n",
    "        parapen = np.zeros(X.shape[1])\n",
    "    # sanity check\n",
    "    assert (len(parapen) == X.shape[1]), \"wrong length for parapen\"\n",
    "    penmat = np.diag(parapen*lam) \n",
    "    b = np.dot(np.linalg.inv(np.dot(X.T, X)+ penmat), np.dot(X.T, y))\n",
    "    return(b)\n",
    "\n",
    "# function to get FE estimates\n",
    "def getfe(b, GMx, GMy):\n",
    "    fe = GMy - np.dot(GMx, b)\n",
    "    return fe\n",
    "\n",
    "# mse function\n",
    "def mse(x, y):\n",
    "    return ((x-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPNN(nn.Module):\n",
    "    def __init__(self, fe_in_dim, P_in_dim=0, nnodes=None, nlayers = 10, \n",
    "                 NP_out_dim=1, NP_in_dim=365 * 4, dropout = 0.0,\n",
    "                batch_normalize=True):\n",
    "        super(SPNN, self).__init__()\n",
    "        \n",
    "        if nnodes is None:\n",
    "            nnodes = np.repeat(25, nlayers)\n",
    "        if nnodes[-1] != NP_out_dim:\n",
    "            nnodes[-1] = nnodes[-1]**0 * NP_out_dim\n",
    "        \n",
    "        self.lin1 = nn.Linear(NP_in_dim, nnodes[0])\n",
    "        self.bn_1 = nn.BatchNorm1d(num_features=nnodes[0])\n",
    "        self.linears = nn.ModuleList([nn.Linear(nnodes[i], nnodes[i + 1]) for i in range(nlayers - 1)])\n",
    "        self.bn_list = nn.ModuleList([nn.BatchNorm1d(num_features=nnodes[i+1]) for i in range(nlayers - 1)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lin_fe = nn.Linear(fe_in_dim, 1, bias=False)\n",
    "        cat_dim = 1 + NP_out_dim + P_in_dim\n",
    "        self.linout = nn.Linear(cat_dim, 1, bias=False)\n",
    "        self.batch_normalize = batch_normalize\n",
    "        self.params = {'nnodes': nnodes, 'nlayers': nlayers, 'NP_out_dim': NP_out_dim, 'NP_in_dim': NP_in_dim,\n",
    "                      'batch_normalize': batch_normalize}\n",
    "\n",
    "    def forward(self, NP, FE, P = None):\n",
    "        # Find a way to do batch normalization here\n",
    "        x = self.lin1(NP)\n",
    "        x = F.relu(self.dropout(x))\n",
    "        if self.batch_normalize:\n",
    "            x = self.bn_1(x)\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = F.relu(self.dropout(l(x)))\n",
    "            if self.batch_normalize:\n",
    "                x = self.bn_list[i](x)\n",
    "        fe = self.lin_fe(FE)\n",
    "        if P is None:\n",
    "            concat = torch.cat((fe, x), dim=1)\n",
    "        else:\n",
    "            concat = torch.cat((fe, P, x), dim=1)\n",
    "        return self.linout(concat)\n",
    "    \n",
    "    def top_layer(self, NP, FE, P = None):\n",
    "        x = self.lin1(NP)\n",
    "        x = F.relu(self.dropout(x))\n",
    "        if self.batch_normalize:\n",
    "            x = self.bn_1(x)\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = F.relu(self.dropout(l(x)))\n",
    "            if self.batch_normalize:\n",
    "                x = self.bn_list[i](x)\n",
    "        if P is None:\n",
    "            return x\n",
    "        else:\n",
    "            return torch.cat((P, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_data(data, P=None):\n",
    "    if P is None:\n",
    "        _NP, _FE, _Y = data\n",
    "        _P = None\n",
    "    else:\n",
    "        _NP, _FE, _P, _Y = data\n",
    "    return (_NP, _FE, _P, _Y)\n",
    "\n",
    "def test_SPNN(net, NP, fe, y, P):\n",
    "    FE = torch.tensor(dmatrices(\"1~ C(fips)-1\", fe)[1])\n",
    "    Y = torch.tensor(y.values)\n",
    "    NP_in_dim = NP.size()[1]\n",
    "    FE_in_dim = FE.size()[1]\n",
    "    if P is None:\n",
    "        P_in_dim = 0\n",
    "        test_dataset = torch.utils.data.TensorDataset(NP, FE, Y)  \n",
    "    else:\n",
    "        P_in_dim = P.size()[1]\n",
    "        test_dataset = torch.utils.data.TensorDataset(NP, FE, P, Y)  \n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100,\n",
    "                                            shuffle=False, num_workers=1)\n",
    "    with torch.no_grad():\n",
    "            net.eval()\n",
    "    predictions = None\n",
    "    actual = None\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(testloader, 0):\n",
    "            _NP, _FE, _P, _Y = unpack_data(data, P)\n",
    "            batch_predictions = net(_NP, _FE, _P).data.numpy()\n",
    "            if predictions is None:\n",
    "                predictions = batch_predictions\n",
    "            else:\n",
    "                predictions = np.append(predictions, batch_predictions)\n",
    "            if actual is None:\n",
    "                actual = _Y.numpy()\n",
    "            else:\n",
    "                actual = np.append(actual, _Y.numpy())\n",
    "        print(mse(predictions, actual))\n",
    "\n",
    "def fit_with_ols(y, P, NP, fe, # input data\n",
    "                 dropout, batch_normalize, # tunable hyperparameters\n",
    "                 batch_size, epochs, LR, # training parameter\n",
    "                 nnodes, NP_out_dim, nlayers, # architecture\n",
    "                 verbose=True):\n",
    "    FE = torch.tensor(dmatrices(\"1~ C(fips)-1\", fe)[1])\n",
    "    Y = torch.tensor(y.values)\n",
    "    NP_in_dim = NP.size()[1]\n",
    "    FE_in_dim = FE.size()[1]\n",
    "    if P is None:\n",
    "        P_in_dim = 0\n",
    "        train_dataset = torch.utils.data.TensorDataset(NP, FE, Y)\n",
    "    else:\n",
    "        P_in_dim = P.size()[1]\n",
    "        train_dataset = torch.utils.data.TensorDataset(NP, FE, P, Y)  \n",
    "    \n",
    "    net = SPNN(fe_in_dim=FE_in_dim, P_in_dim=P_in_dim, nnodes=nnodes, nlayers = nlayers, \n",
    "                 NP_out_dim=NP_out_dim, NP_in_dim=NP_in_dim, dropout = dropout,\n",
    "                batch_normalize=batch_normalize)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=1)\n",
    "    start_time = timeit.default_timer()\n",
    "    for epoch in range(epochs): \n",
    "        running_loss = 0.0\n",
    "        net = net.double() \n",
    "        net.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            _NP, _FE, _P, _Y = unpack_data(data, P)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(_NP, _FE, _P)\n",
    "            loss = criterion(outputs, _Y.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += float(loss)\n",
    "        if verbose: print(\"Epoch \" + str(epoch + 1) + \", mse \" + str(running_loss / (i + 1)))  \n",
    "    # This section implements the 'OLS trick'\n",
    "    with torch.no_grad():\n",
    "        top_layer_inputs = net.top_layer(NP=NP, FE=FE).numpy()\n",
    "        xbar = groupmeans(fe, pd.DataFrame(top_layer_inputs))\n",
    "        ybar = groupmeans(fe, y)\n",
    "        newb = ols(X = top_layer_inputs - xbar, \n",
    "                y = np.array(y) - ybar)\n",
    "        alpha = getfe(newb, xbar, ybar)\n",
    "        alpha = pd.concat([fe, pd.DataFrame(alpha)], axis = 1).groupby(by = \"fips\").mean()\n",
    "        net.linout.weight = torch.nn.parameter.Parameter(torch.tensor(np.concatenate((np.ones((1, 1)), newb), axis=1)))\n",
    "        net.lin_fe.weight = torch.nn.parameter.Parameter(torch.tensor(alpha.values.reshape(1, -1)))\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    if verbose: \n",
    "        print('MSE after OLS trick:')\n",
    "        test_SPNN(net, NP, fe, y, P)\n",
    "        print('Elapsed time, seconds: ' + str(elapsed))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mse 19.580916456941093\n",
      "Epoch 2, mse 11.563215538086116\n",
      "Epoch 3, mse 2.5104541617369356\n",
      "Epoch 4, mse 0.09825290351995082\n",
      "Epoch 5, mse 0.03176246844376008\n",
      "MSE after OLS trick:\n",
      "0.029349656683673262\n",
      "Elapsed time, seconds: 2.5589742510346696\n",
      "MSE on test set:\n",
      "0.02893234303953883\n"
     ]
    }
   ],
   "source": [
    "random=1234\n",
    "torch.manual_seed(random + 2)\n",
    "np.random.seed(random * 3)\n",
    "rn.seed(random * 3)\n",
    "NP = torch.tensor(nonparametric.values)\n",
    "tr_idx, te_idx = train_test_split(range(len(y)), test_size=0.3)\n",
    "# tr_idx = np.where(df.year < 2005)[0]\n",
    "# te_idx = np.where(df.year >= 2005)[0]\n",
    "net = fit_with_ols(y=y.iloc[tr_idx, :], P=None, NP=NP[tr_idx, :], fe=fe.iloc[tr_idx, :], # input data\n",
    "            dropout = 0.0, batch_normalize = True, \n",
    "            batch_size = 100, epochs = 5, LR = 0.01,\n",
    "            nnodes = None, NP_out_dim = 1, nlayers = 10,\n",
    "            verbose=True)\n",
    "print(\"MSE on test set:\")\n",
    "test_SPNN(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mse 19.580916456941093\n",
      "Epoch 2, mse 11.563215538086116\n",
      "Epoch 3, mse 2.5104541617369356\n",
      "Epoch 4, mse 0.09825290351995082\n",
      "Epoch 5, mse 0.03176246844376008\n",
      "Epoch 6, mse 0.0259762000172062\n",
      "Epoch 7, mse 0.021354045353752037\n",
      "Epoch 8, mse 0.017580332189366304\n",
      "Epoch 9, mse 0.018034091051415884\n",
      "Epoch 10, mse 0.016878595772360658\n",
      "MSE after OLS trick:\n",
      "0.023520504886932664\n",
      "Elapsed time, seconds: 5.072139465017244\n",
      "MSE on test set:\n",
      "0.020961691171938072\n"
     ]
    }
   ],
   "source": [
    "random=1234\n",
    "torch.manual_seed(random + 2)\n",
    "np.random.seed(random * 3)\n",
    "rn.seed(random * 3)\n",
    "NP = torch.tensor(nonparametric.values)\n",
    "tr_idx, te_idx = train_test_split(range(len(y)), test_size=0.3)\n",
    "# tr_idx = np.where(df.year < 2005)[0]\n",
    "# te_idx = np.where(df.year >= 2005)[0]\n",
    "net = fit_with_ols(y=y.iloc[tr_idx, :], P=None, NP=NP[tr_idx, :], fe=fe.iloc[tr_idx, :], # input data\n",
    "            dropout = 0.0, batch_normalize = True, \n",
    "            batch_size = 100, epochs = 10, LR = 0.01,\n",
    "            nnodes = None, NP_out_dim = 1, nlayers = 10,\n",
    "            verbose=True)\n",
    "print(\"MSE on test set:\")\n",
    "test_SPNN(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mse 19.580916456941093\n",
      "Epoch 2, mse 11.563215538086116\n",
      "Epoch 3, mse 2.5104541617369356\n",
      "Epoch 4, mse 0.09825290351995082\n",
      "Epoch 5, mse 0.03176246844376008\n",
      "Epoch 6, mse 0.0259762000172062\n",
      "Epoch 7, mse 0.021354045353752037\n",
      "Epoch 8, mse 0.017580332189366304\n",
      "Epoch 9, mse 0.018034091051415884\n",
      "Epoch 10, mse 0.016878595772360658\n",
      "MSE after OLS trick:\n",
      "0.023520504886932664\n",
      "Elapsed time, seconds: 5.651581092970446\n",
      "MSE on test set:\n",
      "0.020961691171938072\n"
     ]
    }
   ],
   "source": [
    "random=1234\n",
    "torch.manual_seed(random + 2)\n",
    "np.random.seed(random * 3)\n",
    "rn.seed(random * 3)\n",
    "tr_idx, te_idx = train_test_split(range(len(y)), test_size=0.3)\n",
    "# tr_idx = np.where(df.year < 2005)[0]\n",
    "# te_idx = np.where(df.year >= 2005)[0]\n",
    "net = fit_with_ols(y=y.iloc[tr_idx, :], P=None, NP=NP[tr_idx, :], fe=fe.iloc[tr_idx, :], # input data\n",
    "            dropout = 0.0, batch_normalize = True, \n",
    "            batch_size = 100, epochs = 10, LR = 0.01,\n",
    "            nnodes = None, NP_out_dim = 1, nlayers = 10,\n",
    "            verbose=True)\n",
    "print(\"MSE on test set:\")\n",
    "test_SPNN(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mse 18.18769191661797\n",
      "Epoch 2, mse 6.9732871473758715\n",
      "Epoch 3, mse 0.34330591483491707\n",
      "Epoch 4, mse 0.03226341017972963\n",
      "Epoch 5, mse 0.020035450014373388\n",
      "Epoch 6, mse 0.01713831098743937\n",
      "Epoch 7, mse 0.016529824488094695\n",
      "Epoch 8, mse 0.015980792028404193\n",
      "Epoch 9, mse 0.014479306269528106\n",
      "Epoch 10, mse 0.014507535048611927\n",
      "MSE after OLS trick:\n",
      "0.010809162544461058\n",
      "Elapsed time, seconds: 6.165436386014335\n",
      "MSE on test set:\n",
      "0.22580215550244265\n"
     ]
    }
   ],
   "source": [
    "random=1234\n",
    "torch.manual_seed(random + 2)\n",
    "np.random.seed(random * 3)\n",
    "rn.seed(random * 3)\n",
    "# tr_idx, te_idx = train_test_split(range(len(y)), test_size=0.3)\n",
    "tr_idx = np.where(df.year < 2005)[0]\n",
    "te_idx = np.where(df.year >= 2005)[0]\n",
    "net = fit_with_ols(y=y.iloc[tr_idx, :], P=None, NP=NP[tr_idx, :], fe=fe.iloc[tr_idx, :], # input data\n",
    "            dropout = 0.0, batch_normalize = True, \n",
    "            batch_size = 100, epochs = 10, LR = 0.01,\n",
    "            nnodes = None, NP_out_dim = 1, nlayers = 10,\n",
    "            verbose=True)\n",
    "print(\"MSE on test set:\")\n",
    "test_SPNN(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
