{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation of Crane-Droesch's algorithm in pytorch, for better reproducibility.\n",
    "See the original paper [here](https://iopscience.iop.org/article/10.1088/1748-9326/aae159)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rn\n",
    "from patsy import dmatrices\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fips</th>\n",
       "      <th>year</th>\n",
       "      <th>cornyield</th>\n",
       "      <th>cottonyield</th>\n",
       "      <th>soyyield</th>\n",
       "      <th>prec_101</th>\n",
       "      <th>prec_102</th>\n",
       "      <th>prec_103</th>\n",
       "      <th>prec_104</th>\n",
       "      <th>prec_105</th>\n",
       "      <th>...</th>\n",
       "      <th>tMax_1223</th>\n",
       "      <th>tMax_1224</th>\n",
       "      <th>tMax_1225</th>\n",
       "      <th>tMax_1226</th>\n",
       "      <th>tMax_1227</th>\n",
       "      <th>tMax_1228</th>\n",
       "      <th>tMax_1229</th>\n",
       "      <th>tMax_1230</th>\n",
       "      <th>tMax_1231</th>\n",
       "      <th>logcornyield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19001</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>36.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.6</td>\n",
       "      <td>0.012033</td>\n",
       "      <td>0.589944</td>\n",
       "      <td>0.016872</td>\n",
       "      <td>0.013338</td>\n",
       "      <td>0.012083</td>\n",
       "      <td>...</td>\n",
       "      <td>-7.547232</td>\n",
       "      <td>-10.325850</td>\n",
       "      <td>-7.136567</td>\n",
       "      <td>-7.685323</td>\n",
       "      <td>-6.439754</td>\n",
       "      <td>1.537972</td>\n",
       "      <td>6.898899</td>\n",
       "      <td>4.773990</td>\n",
       "      <td>2.318576</td>\n",
       "      <td>3.602777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19003</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>29.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.4</td>\n",
       "      <td>0.012352</td>\n",
       "      <td>0.466833</td>\n",
       "      <td>0.017530</td>\n",
       "      <td>0.014132</td>\n",
       "      <td>0.012457</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.751188</td>\n",
       "      <td>-8.839404</td>\n",
       "      <td>-7.620637</td>\n",
       "      <td>-7.860257</td>\n",
       "      <td>-6.613001</td>\n",
       "      <td>1.977630</td>\n",
       "      <td>5.981259</td>\n",
       "      <td>4.881736</td>\n",
       "      <td>2.989506</td>\n",
       "      <td>3.377588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19005</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>49.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.010945</td>\n",
       "      <td>0.290628</td>\n",
       "      <td>0.375549</td>\n",
       "      <td>0.081220</td>\n",
       "      <td>0.431489</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.687468</td>\n",
       "      <td>-13.181018</td>\n",
       "      <td>-5.470778</td>\n",
       "      <td>-8.814853</td>\n",
       "      <td>-11.140276</td>\n",
       "      <td>-1.914674</td>\n",
       "      <td>3.245067</td>\n",
       "      <td>2.077545</td>\n",
       "      <td>2.868546</td>\n",
       "      <td>3.899950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19007</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.9</td>\n",
       "      <td>0.011618</td>\n",
       "      <td>0.578579</td>\n",
       "      <td>0.711192</td>\n",
       "      <td>0.190579</td>\n",
       "      <td>0.011618</td>\n",
       "      <td>...</td>\n",
       "      <td>-6.857291</td>\n",
       "      <td>-10.774097</td>\n",
       "      <td>-5.085959</td>\n",
       "      <td>-7.094800</td>\n",
       "      <td>-5.920215</td>\n",
       "      <td>2.955707</td>\n",
       "      <td>5.388830</td>\n",
       "      <td>4.523667</td>\n",
       "      <td>10.219614</td>\n",
       "      <td>3.367296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19009</td>\n",
       "      <td>1951.0</td>\n",
       "      <td>50.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>24.5</td>\n",
       "      <td>0.009158</td>\n",
       "      <td>0.599447</td>\n",
       "      <td>0.009177</td>\n",
       "      <td>0.009167</td>\n",
       "      <td>0.009159</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.321472</td>\n",
       "      <td>-10.493649</td>\n",
       "      <td>-8.044870</td>\n",
       "      <td>-8.837232</td>\n",
       "      <td>-7.460020</td>\n",
       "      <td>-0.166930</td>\n",
       "      <td>5.955386</td>\n",
       "      <td>4.289409</td>\n",
       "      <td>1.768050</td>\n",
       "      <td>3.916015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1466 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    fips    year  cornyield  cottonyield  soyyield  prec_101  prec_102  \\\n",
       "0  19001  1951.0       36.7          NaN      21.6  0.012033  0.589944   \n",
       "1  19003  1951.0       29.3          NaN      16.4  0.012352  0.466833   \n",
       "2  19005  1951.0       49.4          NaN      12.5  0.010945  0.290628   \n",
       "3  19007  1951.0       29.0          NaN      18.9  0.011618  0.578579   \n",
       "4  19009  1951.0       50.2          NaN      24.5  0.009158  0.599447   \n",
       "\n",
       "   prec_103  prec_104  prec_105      ...       tMax_1223  tMax_1224  \\\n",
       "0  0.016872  0.013338  0.012083      ...       -7.547232 -10.325850   \n",
       "1  0.017530  0.014132  0.012457      ...       -6.751188  -8.839404   \n",
       "2  0.375549  0.081220  0.431489      ...      -12.687468 -13.181018   \n",
       "3  0.711192  0.190579  0.011618      ...       -6.857291 -10.774097   \n",
       "4  0.009177  0.009167  0.009159      ...       -9.321472 -10.493649   \n",
       "\n",
       "   tMax_1225  tMax_1226  tMax_1227  tMax_1228  tMax_1229  tMax_1230  \\\n",
       "0  -7.136567  -7.685323  -6.439754   1.537972   6.898899   4.773990   \n",
       "1  -7.620637  -7.860257  -6.613001   1.977630   5.981259   4.881736   \n",
       "2  -5.470778  -8.814853 -11.140276  -1.914674   3.245067   2.077545   \n",
       "3  -5.085959  -7.094800  -5.920215   2.955707   5.388830   4.523667   \n",
       "4  -8.044870  -8.837232  -7.460020  -0.166930   5.955386   4.289409   \n",
       "\n",
       "   tMax_1231  logcornyield  \n",
       "0   2.318576      3.602777  \n",
       "1   2.989506      3.377588  \n",
       "2   2.868546      3.899950  \n",
       "3  10.219614      3.367296  \n",
       "4   1.768050      3.916015  \n",
       "\n",
       "[5 rows x 1466 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base = \"/Users/max/Documents/climate_and_agriculture/code/\" # set your own directory here\n",
    "df = pd.read_csv(base + \"data/iowa_weather.csv\")\n",
    "df = df.drop(columns = 'Unnamed: 0')\n",
    "field = 'cornyield'\n",
    "df['log' + field] = np.log(df[field])\n",
    "df = df.dropna(axis=0, subset=['log'+ field])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the covariates\n",
    "y = df[[\"log\" + field]]\n",
    "fe = df[[\"fips\"]]\n",
    "parametric = None\n",
    "nonparametric = df[df.columns[pd.Series(df.columns).str.contains(\"_\")]]\n",
    "NP = torch.tensor(nonparametric.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupmeans(G, X):\n",
    "    gm = pd.concat([G, X], axis = 1).groupby(G.columns[0], axis = 0).mean()\n",
    "    GM = np.empty(shape = X.shape)\n",
    "    for j in np.unique(G.values):\n",
    "        idx = np.where(j == G.values)[0]\n",
    "        toput = np.repeat(gm.loc[j,].values.reshape(1,GM.shape[1]), repeats = len(idx), axis = 0)\n",
    "        GM[idx,:] = toput\n",
    "    return GM\n",
    "\n",
    "# ols function\n",
    "def ols(X, y, lam = 0, parapen = None):\n",
    "    # default value is unpenalized\n",
    "    if parapen is None:\n",
    "        parapen = np.zeros(X.shape[1])\n",
    "    # sanity check\n",
    "    assert (len(parapen) == X.shape[1]), \"wrong length for parapen\"\n",
    "    penmat = np.diag(parapen*lam) \n",
    "    b = np.dot(np.linalg.inv(np.dot(X.T, X)+ penmat), np.dot(X.T, y))\n",
    "    return(b)\n",
    "\n",
    "# function to get FE estimates\n",
    "def getfe(b, GMx, GMy):\n",
    "    fe = GMy - np.dot(GMx, b)\n",
    "    return fe\n",
    "\n",
    "# mse function\n",
    "def mse(x, y):\n",
    "    return ((x-y)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPNN(nn.Module):\n",
    "    def __init__(self, fe_in_dim, P_in_dim=0, nnodes=None, nlayers = 10, \n",
    "                 NP_out_dim=1, NP_in_dim=365 * 4, dropout = 0.0,\n",
    "                batch_normalize=True):\n",
    "        super(SPNN, self).__init__()\n",
    "        \n",
    "        if nnodes is None:\n",
    "            nnodes = np.repeat(25, nlayers)\n",
    "        if nnodes[-1] != NP_out_dim:\n",
    "            nnodes[-1] = nnodes[-1]**0 * NP_out_dim\n",
    "        \n",
    "        self.lin1 = nn.Linear(NP_in_dim, nnodes[0])\n",
    "        self.bn_1 = nn.BatchNorm1d(num_features=nnodes[0])\n",
    "        self.linears = nn.ModuleList([nn.Linear(nnodes[i], nnodes[i + 1]) for i in range(nlayers - 1)])\n",
    "        self.bn_list = nn.ModuleList([nn.BatchNorm1d(num_features=nnodes[i+1]) for i in range(nlayers - 1)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lin_fe = nn.Linear(fe_in_dim, 1, bias=False)\n",
    "        cat_dim = 1 + NP_out_dim + P_in_dim\n",
    "        self.linout = nn.Linear(cat_dim, 1, bias=False)\n",
    "        self.batch_normalize = batch_normalize\n",
    "        self.params = {'nnodes': nnodes, 'nlayers': nlayers, 'NP_out_dim': NP_out_dim, 'NP_in_dim': NP_in_dim,\n",
    "                      'batch_normalize': batch_normalize}\n",
    "\n",
    "    def forward(self, NP, FE, P = None):\n",
    "        # Find a way to do batch normalization here\n",
    "        x = self.lin1(NP)\n",
    "        x = F.relu(self.dropout(x))\n",
    "        if self.batch_normalize:\n",
    "            x = self.bn_1(x)\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = F.relu(self.dropout(l(x)))\n",
    "            if self.batch_normalize:\n",
    "                x = self.bn_list[i](x)\n",
    "        fe = self.lin_fe(FE)\n",
    "        if P is None:\n",
    "            concat = torch.cat((fe, x), dim=1)\n",
    "        else:\n",
    "            concat = torch.cat((fe, P, x), dim=1)\n",
    "        return self.linout(concat)\n",
    "    \n",
    "    def top_layer(self, NP, FE, P = None):\n",
    "        x = self.lin1(NP)\n",
    "        x = F.relu(self.dropout(x))\n",
    "        if self.batch_normalize:\n",
    "            x = self.bn_1(x)\n",
    "        for i, l in enumerate(self.linears):\n",
    "            x = F.relu(self.dropout(l(x)))\n",
    "            if self.batch_normalize:\n",
    "                x = self.bn_list[i](x)\n",
    "        if P is None:\n",
    "            return x\n",
    "        else:\n",
    "            return torch.cat((P, x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack_data(data, P=None):\n",
    "    if P is None:\n",
    "        _NP, _FE, _Y = data\n",
    "        _P = None\n",
    "    else:\n",
    "        _NP, _FE, _P, _Y = data\n",
    "    return (_NP, _FE, _P, _Y)\n",
    "\n",
    "def predict(net, NP, fe, y, P):\n",
    "    FE = torch.tensor(dmatrices(\"1~ C(fips)-1\", fe)[1])\n",
    "    Y = torch.tensor(y.values)\n",
    "    NP_in_dim = NP.size()[1]\n",
    "    FE_in_dim = FE.size()[1]\n",
    "    if P is None:\n",
    "        P_in_dim = 0\n",
    "        dataset = torch.utils.data.TensorDataset(NP, FE, Y)  \n",
    "    else:\n",
    "        P_in_dim = P.size()[1]\n",
    "        dataset = torch.utils.data.TensorDataset(NP, FE, P, Y)  \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=100,\n",
    "                                            shuffle=False, num_workers=1)\n",
    "    return predict_from_loader(loader, net, P)\n",
    "\n",
    "def predict_from_loader(loader, net, P):\n",
    "    net.eval()\n",
    "    predictions = None\n",
    "    actual = None\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(loader, 0):\n",
    "            _NP, _FE, _P, _Y = unpack_data(data, P)\n",
    "            batch_predictions = net(_NP, _FE, _P).data.numpy()\n",
    "            if predictions is None:\n",
    "                predictions = batch_predictions\n",
    "            else:\n",
    "                predictions = np.append(predictions, batch_predictions)\n",
    "            if actual is None:\n",
    "                actual = _Y.numpy()\n",
    "            else:\n",
    "                actual = np.append(actual, _Y.numpy())\n",
    "    return actual, predictions\n",
    "\n",
    "def test_SPNN(net, NP, fe, y, P):\n",
    "    FE = torch.tensor(dmatrices(\"1~ C(fips)-1\", fe)[1])\n",
    "    Y = torch.tensor(y.values)\n",
    "    NP_in_dim = NP.size()[1]\n",
    "    FE_in_dim = FE.size()[1]\n",
    "    if P is None:\n",
    "        P_in_dim = 0\n",
    "        test_dataset = torch.utils.data.TensorDataset(NP, FE, Y)  \n",
    "    else:\n",
    "        P_in_dim = P.size()[1]\n",
    "        test_dataset = torch.utils.data.TensorDataset(NP, FE, P, Y)  \n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=100,\n",
    "                                            shuffle=False, num_workers=1)\n",
    "    actual, predictions = predict_from_loader(testloader, net, P)\n",
    "    print(mse(actual, predictions))\n",
    "\n",
    "def fit_with_ols(y, P, NP, fe, # input data\n",
    "                 dropout, batch_normalize, # tunable hyperparameters\n",
    "                 batch_size, epochs, LR, # training parameter\n",
    "                 nnodes, NP_out_dim, nlayers, # architecture\n",
    "                 verbose=True):\n",
    "    FE = torch.tensor(dmatrices(\"1~ C(fips)-1\", fe)[1])\n",
    "    Y = torch.tensor(y.values)\n",
    "    NP_in_dim = NP.size()[1]\n",
    "    FE_in_dim = FE.size()[1]\n",
    "    if P is None:\n",
    "        P_in_dim = 0\n",
    "        train_dataset = torch.utils.data.TensorDataset(NP, FE, Y)\n",
    "    else:\n",
    "        P_in_dim = P.size()[1]\n",
    "        train_dataset = torch.utils.data.TensorDataset(NP, FE, P, Y)  \n",
    "    \n",
    "    net = SPNN(fe_in_dim=FE_in_dim, P_in_dim=P_in_dim, nnodes=nnodes, nlayers = nlayers, \n",
    "                 NP_out_dim=NP_out_dim, NP_in_dim=NP_in_dim, dropout = dropout,\n",
    "                batch_normalize=batch_normalize)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=LR)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                             shuffle=True, num_workers=1)\n",
    "    start_time = timeit.default_timer()\n",
    "    for epoch in range(epochs): \n",
    "        running_loss = 0.0\n",
    "        net = net.double() \n",
    "        net.train()\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            _NP, _FE, _P, _Y = unpack_data(data, P)\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(_NP, _FE, _P)\n",
    "            loss = criterion(outputs, _Y.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += float(loss)\n",
    "        if verbose: print(\"Epoch \" + str(epoch + 1) + \", mse \" + str(running_loss / (i + 1)))  \n",
    "    # This section implements the 'OLS trick'\n",
    "    with torch.no_grad():\n",
    "        top_layer_inputs = net.top_layer(NP=NP, FE=FE).numpy()\n",
    "        xbar = groupmeans(fe, pd.DataFrame(top_layer_inputs))\n",
    "        ybar = groupmeans(fe, y)\n",
    "        newb = ols(X = top_layer_inputs - xbar, \n",
    "                y = np.array(y) - ybar)\n",
    "        alpha = getfe(newb, xbar, ybar)\n",
    "        alpha = pd.concat([fe, pd.DataFrame(alpha)], axis = 1).groupby(by = \"fips\").mean()\n",
    "        net.linout.weight = torch.nn.parameter.Parameter(torch.tensor(np.concatenate((np.ones((1, 1)), newb), axis=1)))\n",
    "        net.lin_fe.weight = torch.nn.parameter.Parameter(torch.tensor(alpha.values.reshape(1, -1)))\n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    if verbose: \n",
    "        print('MSE after OLS trick:')\n",
    "        test_SPNN(net, NP, fe, y, P)\n",
    "        print('Elapsed time, seconds: ' + str(elapsed))\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mse 19.580916456941093\n",
      "Epoch 2, mse 11.563215538086116\n",
      "Epoch 3, mse 2.5104541617369356\n",
      "Epoch 4, mse 0.09825290351995082\n",
      "Epoch 5, mse 0.03176246844376008\n",
      "MSE after OLS trick:\n",
      "0.029349656683673262\n",
      "Elapsed time, seconds: 2.645678126020357\n",
      "MSE on test set:\n",
      "0.02893234303953883\n"
     ]
    }
   ],
   "source": [
    "random=1234\n",
    "torch.manual_seed(random + 2)\n",
    "np.random.seed(random * 3)\n",
    "rn.seed(random * 3)\n",
    "NP = torch.tensor(nonparametric.values)\n",
    "tr_idx, te_idx = train_test_split(range(len(y)), test_size=0.3)\n",
    "# tr_idx = np.where(df.year < 2005)[0]\n",
    "# te_idx = np.where(df.year >= 2005)[0]\n",
    "net = fit_with_ols(y=y.iloc[tr_idx, :], P=None, NP=NP[tr_idx, :], fe=fe.iloc[tr_idx, :], # input data\n",
    "            dropout = 0.0, batch_normalize = True, \n",
    "            batch_size = 100, epochs = 5, LR = 0.01,\n",
    "            nnodes = None, NP_out_dim = 1, nlayers = 10,\n",
    "            verbose=True)\n",
    "print(\"MSE on test set:\")\n",
    "test_SPNN(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])\n",
    "actual, predicted = predict(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])\n",
    "plt.scatter(actual, predicted)\n",
    "plt.plot(actual, actual)\n",
    "plt.xlabel(\"observed\")\n",
    "plt.ylabel(\"predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mse 19.580916456941093\n",
      "Epoch 2, mse 11.563215538086116\n",
      "Epoch 3, mse 2.5104541617369356\n",
      "Epoch 4, mse 0.09825290351995082\n",
      "Epoch 5, mse 0.03176246844376008\n",
      "Epoch 6, mse 0.0259762000172062\n",
      "Epoch 7, mse 0.021354045353752037\n",
      "Epoch 8, mse 0.017580332189366304\n",
      "Epoch 9, mse 0.018034091051415884\n",
      "Epoch 10, mse 0.016878595772360658\n",
      "MSE after OLS trick:\n",
      "0.023520504886932664\n",
      "Elapsed time, seconds: 5.072139465017244\n",
      "MSE on test set:\n",
      "0.020961691171938072\n"
     ]
    }
   ],
   "source": [
    "random=1234\n",
    "torch.manual_seed(random + 2)\n",
    "np.random.seed(random * 3)\n",
    "rn.seed(random * 3)\n",
    "NP = torch.tensor(nonparametric.values)\n",
    "tr_idx, te_idx = train_test_split(range(len(y)), test_size=0.3)\n",
    "# tr_idx = np.where(df.year < 2005)[0]\n",
    "# te_idx = np.where(df.year >= 2005)[0]\n",
    "net = fit_with_ols(y=y.iloc[tr_idx, :], P=None, NP=NP[tr_idx, :], fe=fe.iloc[tr_idx, :], # input data\n",
    "            dropout = 0.0, batch_normalize = True, \n",
    "            batch_size = 100, epochs = 10, LR = 0.01,\n",
    "            nnodes = None, NP_out_dim = 1, nlayers = 10,\n",
    "            verbose=True)\n",
    "print(\"MSE on test set:\")\n",
    "test_SPNN(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mse 19.580916456941093\n",
      "Epoch 2, mse 11.563215538086116\n",
      "Epoch 3, mse 2.5104541617369356\n",
      "Epoch 4, mse 0.09825290351995082\n",
      "Epoch 5, mse 0.03176246844376008\n",
      "Epoch 6, mse 0.0259762000172062\n",
      "Epoch 7, mse 0.021354045353752037\n",
      "Epoch 8, mse 0.017580332189366304\n",
      "Epoch 9, mse 0.018034091051415884\n",
      "Epoch 10, mse 0.016878595772360658\n",
      "MSE after OLS trick:\n",
      "0.023520504886932664\n",
      "Elapsed time, seconds: 5.651581092970446\n",
      "MSE on test set:\n",
      "0.020961691171938072\n"
     ]
    }
   ],
   "source": [
    "random=1234\n",
    "torch.manual_seed(random + 2)\n",
    "np.random.seed(random * 3)\n",
    "rn.seed(random * 3)\n",
    "tr_idx, te_idx = train_test_split(range(len(y)), test_size=0.3)\n",
    "# tr_idx = np.where(df.year < 2005)[0]\n",
    "# te_idx = np.where(df.year >= 2005)[0]\n",
    "net = fit_with_ols(y=y.iloc[tr_idx, :], P=None, NP=NP[tr_idx, :], fe=fe.iloc[tr_idx, :], # input data\n",
    "            dropout = 0.0, batch_normalize = True, \n",
    "            batch_size = 100, epochs = 10, LR = 0.01,\n",
    "            nnodes = None, NP_out_dim = 1, nlayers = 10,\n",
    "            verbose=True)\n",
    "print(\"MSE on test set:\")\n",
    "test_SPNN(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, mse 18.18769191661797\n",
      "Epoch 2, mse 6.9732871473758715\n",
      "Epoch 3, mse 0.34330591483491707\n",
      "Epoch 4, mse 0.03226341017972963\n",
      "Epoch 5, mse 0.020035450014373388\n",
      "Epoch 6, mse 0.01713831098743937\n",
      "Epoch 7, mse 0.016529824488094695\n",
      "Epoch 8, mse 0.015980792028404193\n",
      "Epoch 9, mse 0.014479306269528106\n",
      "Epoch 10, mse 0.014507535048611927\n",
      "MSE after OLS trick:\n",
      "0.010809162544461058\n",
      "Elapsed time, seconds: 6.165436386014335\n",
      "MSE on test set:\n",
      "0.22580215550244265\n"
     ]
    }
   ],
   "source": [
    "random=1234\n",
    "torch.manual_seed(random + 2)\n",
    "np.random.seed(random * 3)\n",
    "rn.seed(random * 3)\n",
    "# tr_idx, te_idx = train_test_split(range(len(y)), test_size=0.3)\n",
    "tr_idx = np.where(df.year < 2005)[0]\n",
    "te_idx = np.where(df.year >= 2005)[0]\n",
    "net = fit_with_ols(y=y.iloc[tr_idx, :], P=None, NP=NP[tr_idx, :], fe=fe.iloc[tr_idx, :], # input data\n",
    "            dropout = 0.0, batch_normalize = True, \n",
    "            batch_size = 100, epochs = 10, LR = 0.01,\n",
    "            nnodes = None, NP_out_dim = 1, nlayers = 10,\n",
    "            verbose=True)\n",
    "print(\"MSE on test set:\")\n",
    "test_SPNN(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'predicted')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "actual, predicted = predict(net = net, y=y.iloc[te_idx, :], P=None, NP=NP[te_idx, :], fe=fe.iloc[te_idx, :])\n",
    "plt.scatter(actual, predicted)\n",
    "plt.plot(actual, actual)\n",
    "plt.xlabel(\"observed\")\n",
    "plt.ylabel(\"predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-2f4f35ded540>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
